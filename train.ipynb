{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] 2025-08-03 15:15:06,656 deeplotx.embedding : LongformerEncoder initialized on device: cuda.\n"
     ]
    }
   ],
   "source": [
    "from deeplotx import SoftmaxRegression, LongformerEncoder\n",
    "from util import NUM_CLASSES\n",
    "lf_encoder = LongformerEncoder(model_name_or_path='severinsimmler/xlm-roberta-longformer-base-16384')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-03T07:15:06.673238Z",
     "start_time": "2025-08-03T07:14:49.472210Z"
    }
   },
   "id": "f7fac354f36a88bc",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from deeplotx.util import sha256\n",
    "from vortezwohl.cache import LRUCache\n",
    "\n",
    "CACHE = LRUCache(capacity=16384)\n",
    "\n",
    "def encode(text: str) -> torch.Tensor:\n",
    "    key = sha256(text)\n",
    "    if key in CACHE:\n",
    "        return CACHE[key]\n",
    "    emb = lf_encoder.encode(text, cls_only=False).mean(dim=-2, dtype=model.dtype)\n",
    "    CACHE[key] = emb\n",
    "    return emb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-03T07:15:06.689235Z",
     "start_time": "2025-08-03T07:15:06.675236Z"
    }
   },
   "id": "9eaf6bf7092cb96d",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shuffled [['RSI LA1', 2], ['Frosolone', 3], ['estende', 0], ['Grant', 1], ['Beaurains', 3], ['attristé', 0], ['Nankin', 3], ['Salvatore Aurelio', 1], ['Disentis', 3], ['passo del Grimsel', 3], ['Hodotermidae', 4], ['Ангарский Ермак', 2], ['come loro leader', 4], ['West Germany', 3], ['Eastwell Park', 3], ['précipitations occasionnelles', 3], ['Pirgos', 3], ['Bacchanalia', 4], ['Dallas en ,', 2], ['Manuel Pérez Barriopedro', 1], ['SIXAXIS', 4], ['Battle of Salamis', 4], ['LNA', 2]]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./data/datasets.json', 'r', encoding='utf-8') as f:\n",
    "    datasets = json.load(f)\n",
    "    \n",
    "train_dataset = datasets['train']\n",
    "valid_dataset = datasets['valid']\n",
    "test_dataset = datasets['test']\n",
    "f'Dataset loaded'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-03T07:15:54.989669Z",
     "start_time": "2025-08-03T07:15:50.933181Z"
    }
   },
   "id": "85a0011256406ea0",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90ca149f740c3ba0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Model_Name: SoftmaxRegression\n",
      "In_Features: 768\n",
      "Out_Features: 5\n",
      "Device: cuda\n",
      "Dtype: torch.float32\n",
      "Total_Parameters: 49638941\n",
      "Trainable_Parameters: 49638941\n",
      "NonTrainable_Parameters: 0\n",
      "-------------------------------\n",
      "SoftmaxRegression(\n",
      "  (multi_head_ffn_layers): ModuleList(\n",
      "    (0-2): 3 x MultiHeadFeedForward(\n",
      "      (expand_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (ffn_heads): ModuleList(\n",
      "        (0-3): 4 x FeedForward(\n",
      "          (ffn_layers): ModuleList(\n",
      "            (0-1): 2 x FeedForwardUnit(\n",
      "              (up_proj): Linear(in_features=768, out_features=960, bias=True)\n",
      "              (down_proj): Linear(in_features=960, out_features=768, bias=True)\n",
      "              (parametric_relu): PReLU(num_parameters=1)\n",
      "              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (out_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (out_proj): Linear(in_features=768, out_features=5, bias=True)\n",
      ")\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "model = SoftmaxRegression(input_dim=768, output_dim=NUM_CLASSES, num_heads=4, num_layers=3, expansion_factor=1.25, bias=True, dropout_rate=0.2, head_layers=2)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-03T07:16:49.903230Z",
     "start_time": "2025-08-03T07:16:49.811681Z"
    }
   },
   "id": "6b1a645e6818b884",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# train\n",
    "from torch import nn, optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-03T07:16:52.686694Z",
     "start_time": "2025-08-03T07:16:51.779349Z"
    }
   },
   "id": "4bfa8215ab0135b",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_step = 0\n",
    "valid_step = 0\n",
    "writer = SummaryWriter()\n",
    "\n",
    "acc_train_loss = 0.\n",
    "acc_valid_loss = 0.\n",
    "eval_interval = 2000\n",
    "log_interval = 200\n",
    "valid_log_interval = 50"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-03T07:16:56.281557Z",
     "start_time": "2025-08-03T07:16:56.271561Z"
    }
   },
   "id": "26fe594efd09342c",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "shuffle(train_dataset)\n",
    "shuffle(valid_dataset)\n",
    "shuffle(test_dataset)\n",
    "print('Dataset shuffled', list(train_dataset[:23]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90093188c23077a3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded to be a multiple of `config.attention_window`: 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train Step 200 Loss 37.1511185836792 \\\n",
      "Token=riemannianas\n",
      "Pred=[0.1171981692314148, 0.5638753771781921, 0.05842353031039238, 0.2039128988981247, 0.0565900057554245]\n",
      "Label=[1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "- Train Step 400 Loss 36.68031204223633 \\\n",
      "Token=Orla\n",
      "Pred=[0.2138345092535019, 0.31305819749832153, 0.030141593888401985, 0.40460312366485596, 0.03836255148053169]\n",
      "Label=[0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "- Train Step 600 Loss 36.3902938079834 \\\n",
      "Token=Sônia Luyten\n",
      "Pred=[0.054191213101148605, 0.7032058238983154, 0.018815767019987106, 0.2048977017402649, 0.01888944022357464]\n",
      "Label=[0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "- Train Step 800 Loss 36.12401927947998 \\\n",
      "Token=Andrea Noè\n",
      "Pred=[0.036982446908950806, 0.6935858130455017, 0.012155124917626381, 0.2447371929883957, 0.01253949198871851]\n",
      "Label=[0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "- Train Step 1000 Loss 35.87660499572754 \\\n",
      "Token=US-Bürgerin\n",
      "Pred=[0.05050504580140114, 0.41481587290763855, 0.014861099421977997, 0.5023809671401978, 0.017436964437365532]\n",
      "Label=[0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "- Train Step 1200 Loss 35.61079029083252 \\\n",
      "Token=Anders Glenmark\n",
      "Pred=[0.054966218769550323, 0.7537938356399536, 0.008434907533228397, 0.17128676176071167, 0.011518282815814018]\n",
      "Label=[0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "- Train Step 1400 Loss 35.36572652816773 \\\n",
      "Token=Technological Institute\n",
      "Pred=[0.24232837557792664, 0.29874736070632935, 0.01808633655309677, 0.42120248079299927, 0.019635356962680817]\n",
      "Label=[0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "- Train Step 1600 Loss 35.09559362411499 \\\n",
      "Token=Carrozzeria Ghia\n",
      "Pred=[0.1303616166114807, 0.5362451076507568, 0.012408731505274773, 0.3056335151195526, 0.015351003967225552]\n",
      "Label=[0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "- Train Step 1800 Loss 34.83443836212158 \\\n",
      "Token=ALDE\n",
      "Pred=[0.2171141356229782, 0.5462747812271118, 0.009557285346090794, 0.21338847279548645, 0.013665290549397469]\n",
      "Label=[0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "- Valid Step 50 Loss 1.723529725074768 \\\n",
      "Token=Bijen\n",
      "Pred=[0.10109587013721466, 0.271786630153656, 0.00548621965572238, 0.6136998534202576, 0.007931412197649479]\n",
      "Label=[1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "- Valid Step 100 Loss 1.6491523718833923 \\\n",
      "Token=Tommy Nilsson\n",
      "Pred=[0.09765952080488205, 0.3060118556022644, 0.005462877452373505, 0.5832297801971436, 0.007635943125933409]\n",
      "Label=[0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "- Valid Step 150 Loss 1.6770188069343568 \\\n",
      "Token=déclara \"qu\" en\n",
      "Pred=[0.08275387436151505, 0.2795107364654541, 0.005053739529103041, 0.6251782774925232, 0.0075033726170659065]\n",
      "Label=[1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "- Valid Step 200 Loss 1.7100263404846192 \\\n",
      "Token=tacrine\n",
      "Pred=[0.09612879902124405, 0.2862572968006134, 0.005104010924696922, 0.6050305962562561, 0.007479293271899223]\n",
      "Label=[1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "- Valid Step 250 Loss 1.6930509066581727 \\\n",
      "Token=mamona\n",
      "Pred=[0.10351184755563736, 0.27934297919273376, 0.005338345188647509, 0.6037628650665283, 0.008043966256082058]\n",
      "Label=[1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "- Valid Step 300 Loss 1.6955076956748962 \\\n",
      "Token=Honorary\n",
      "Pred=[0.09926603734493256, 0.294148325920105, 0.005522958934307098, 0.5930988788604736, 0.007963815703988075]\n",
      "Label=[1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "- Valid Step 350 Loss 1.6381578874588012 \\\n",
      "Token=счётчиков\n",
      "Pred=[0.10663452744483948, 0.27850010991096497, 0.005694565828889608, 0.6010595560073853, 0.008111282251775265]\n",
      "Label=[1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "- Valid Step 400 Loss 1.6754065871238708 \\\n",
      "Token=Virginie\n",
      "Pred=[0.10224368423223495, 0.2839546203613281, 0.005535146687179804, 0.6003691554069519, 0.00789740588515997]\n",
      "Label=[0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "- Valid Step 450 Loss 1.6769072651863097 \\\n",
      "Token=meertaligheid\n",
      "Pred=[0.09345531463623047, 0.28009507060050964, 0.005193182732909918, 0.6140278577804565, 0.007228553295135498]\n",
      "Label=[1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "- Train Step 2000 Loss 34.55568784713745 \\\n",
      "Token=The Jacket\n",
      "Pred=[0.12004200369119644, 0.26815226674079895, 0.007299311924725771, 0.5941286087036133, 0.010377743281424046]\n",
      "Label=[0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "- Train Step 2200 Loss 34.29494499206543 \\\n",
      "Token=Лосева\n",
      "Pred=[0.1410362869501114, 0.6965212821960449, 0.006081403698772192, 0.14801165461540222, 0.008349301293492317]\n",
      "Label=[0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "- Train Step 2400 Loss 34.02696939468384 \\\n",
      "Token=Barney Phillips\n",
      "Pred=[0.08547727018594742, 0.8111193776130676, 0.0038185741286724806, 0.0937245786190033, 0.005860140547156334]\n",
      "Label=[0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "- Train Step 2600 Loss 33.79630292892456 \\\n",
      "Token=10NES\n",
      "Pred=[0.4940049946308136, 0.3424582779407501, 0.009049896150827408, 0.14161022007465363, 0.012876518070697784]\n",
      "Label=[1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "- Train Step 2800 Loss 33.5348698425293 \\\n",
      "Token=Inside Man\n",
      "Pred=[0.36360445618629456, 0.30941957235336304, 0.009794753976166248, 0.3058205842971802, 0.011360662057995796]\n",
      "Label=[0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "- Train Step 3000 Loss 33.266522827148435 \\\n",
      "Token=Charlotte Flemming\n",
      "Pred=[0.09438289701938629, 0.3665359914302826, 0.006645370740443468, 0.5234049558639526, 0.00903073325753212]\n",
      "Label=[0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "- Train Step 3200 Loss 33.048941421508786 \\\n",
      "Token=John Merrill Knapp\n",
      "Pred=[0.1822490245103836, 0.6658377647399902, 0.004496030043810606, 0.14105741679668427, 0.00635977927595377]\n",
      "Label=[0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "- Train Step 3400 Loss 32.78214460372925 \\\n",
      "Token=Witold Rogoyski\n",
      "Pred=[0.21147817373275757, 0.57768315076828, 0.0051027387380599976, 0.1978212296962738, 0.007914760150015354]\n",
      "Label=[0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "- Train Step 3600 Loss 32.56426330566406 \\\n",
      "Token=Beachwood Canyon\n",
      "Pred=[0.15939681231975555, 0.5982274413108826, 0.006533134263008833, 0.22642692923545837, 0.009415751323103905]\n",
      "Label=[0.0, 0.0, 0.0, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from util import one_hot\n",
    "\n",
    "elastic_net_param = {\n",
    "    'alpha': 2e-4,\n",
    "    'rho': 0.2\n",
    "}\n",
    "learning_rate = 2e-6\n",
    "num_epochs = 1500\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (_token, _label) in enumerate(train_dataset):\n",
    "        _one_hot_label = one_hot(_label).to(model.dtype).to(model.device)\n",
    "        outputs = model.forward(encode(_token))\n",
    "        loss = loss_function(outputs, _one_hot_label) + model.elastic_net(alpha=elastic_net_param['alpha'], rho=elastic_net_param['rho'])\n",
    "        acc_train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if train_step % log_interval == 0 and train_step > 0:\n",
    "            writer.add_scalar('train/loss', acc_train_loss / log_interval, train_step)\n",
    "            print(f'- Train Step {train_step} Loss {acc_train_loss / log_interval} \\\\'\n",
    "                  f'\\nToken={_token}'\n",
    "                  f'\\nPred={outputs.tolist()}'\n",
    "                  f'\\nLabel={_one_hot_label.tolist()}', flush=True)\n",
    "            acc_train_loss = 0.\n",
    "        train_step += 1\n",
    "        if train_step % eval_interval == 0:\n",
    "            model.eval()\n",
    "            rand_idx = randint(0, len(valid_dataset) - 501)\n",
    "            with torch.no_grad():\n",
    "                for _i, (__token, __label) in enumerate(valid_dataset[rand_idx: rand_idx + 500]):\n",
    "                    _one_hot_label = one_hot(__label).to(model.dtype).to(model.device)\n",
    "                    outputs = model.forward(encode(__token))\n",
    "                    loss = loss_function(outputs, _one_hot_label)\n",
    "                    acc_valid_loss += loss.item()\n",
    "                    if valid_step % valid_log_interval == 0 and valid_step > 0:\n",
    "                        writer.add_scalar('valid/loss', acc_valid_loss / valid_log_interval, valid_step)\n",
    "                        print(f'- Valid Step {valid_step} Loss {acc_valid_loss / valid_log_interval} \\\\'\n",
    "                              f'\\nToken={__token}'\n",
    "                              f'\\nPred={outputs.tolist()}'\n",
    "                              f'\\nLabel={_one_hot_label.tolist()}', flush=True)\n",
    "                        acc_valid_loss = 0.\n",
    "                    valid_step += 1\n",
    "            model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-08-03T07:16:57.533858Z"
    }
   },
   "id": "9bb8c54cf0d8705c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.eval()\n",
    "'''\n",
    "0: other\n",
    "1: person\n",
    "2: org\n",
    "3: loc\n",
    "4: misc\n",
    "'''\n",
    "test_tokens = ['Mike', 'John', 'Smith', 'London', 'NYC', 'HongKong', 'China', 'South Africa', 'Korea', 'Fruit', 'BMW', 'Mother', 'Friend', 'Cheer leader']\n",
    "with torch.no_grad():\n",
    "    for _tok in test_tokens:\n",
    "        _dist = model.forward(encode(_tok))\n",
    "        print(f'Token={_tok}, Class={torch.argmax(_dist)}', flush=True)\n",
    "model.train()\n",
    "'Test finished.'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "683889b3c2e6c744",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
